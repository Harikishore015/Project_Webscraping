{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookscraper(url):\n",
    "\n",
    "    '''\n",
    "    Gets all information of a specific book\n",
    "    :param url: url of one book\n",
    "    :return: dictionary with all information of a book\n",
    "    '''\n",
    "    page_book = requests.get(url)\n",
    "    soup_book = BeautifulSoup(page_book.content, 'html.parser')\n",
    "\n",
    "    title = None\n",
    "    author = None\n",
    "    num_reviews = None\n",
    "    num_ratings = None\n",
    "    avg_rating = None\n",
    "    num_pages = None\n",
    "    original_publish_year = None\n",
    "    genres = None\n",
    "    awards = None\n",
    "    places = None\n",
    "\n",
    "    try:\n",
    "        title = soup_book.find(\"h1\").text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        author = soup_book.find(\"span\", itemprop=\"name\").text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        num_reviews = int(soup_book.find(\"meta\", itemprop=\"reviewCount\").get('content'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        num_ratings = int(soup_book.find(\"meta\", itemprop=\"ratingCount\").get('content'))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        avg_rating = float(soup_book.find(\"span\", itemprop=\"ratingValue\").text.strip())\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        num_pages = int(soup_book.find(\"span\", itemprop=\"numberOfPages\").text.strip().split(' ')[0])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        details = soup_book.find(\"div\", id=\"details\")\n",
    "        original_publish_year = details.find_all(\"div\", class_=\"row\")[-1].text.strip().split(' ')\n",
    "        for element in original_publish_year:\n",
    "            try:\n",
    "                original_publish_year = int(element)\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        series = soup_book.find(\"h2\", id=\"bookSeries\").text.strip()\n",
    "        if series.strip() == \"\" or series == None:\n",
    "            series = 0\n",
    "        else:\n",
    "            series = 1\n",
    "    except:\n",
    "        series = 0\n",
    "    try:\n",
    "        genres_list = soup_book.find_all(\"a\", class_=\"actionLinkLite bookPageGenreLink\")\n",
    "        genres = \";\".join([genre.text for genre in genres_list])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        awards_list = soup_book.find_all(\"a\", class_=\"award\")\n",
    "        awards = \";\".join([award.text for award in awards_list])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        places_detail = details.find(\"div\",id=\"bookDataBox\").findChildren(\"div\", recursive=False)\n",
    "        i=0\n",
    "        while i<len(places_detail):\n",
    "            if places_detail[i].text == \"Setting\":\n",
    "                places_html = places_detail[i+1]\n",
    "                break\n",
    "            i += 1\n",
    "        places = ';'.join([place.text for place in places_html.find_all('a')])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    to_append = {\"url\": url , \"title\":title,\"author\":author,\"num_reviews\":num_reviews,\"num_ratings\":num_ratings,\"avg_rating\":avg_rating,\\\n",
    "                 \"num_pages\":num_pages,\"original_publish_year\":original_publish_year,\"series\":series,\"genres\":genres,\"awards\":awards,\"places\":places}\n",
    "    return to_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper():\n",
    "    '''\n",
    "    Loop all best Dystopian_and_Post_Apocalyptic_Fiction books in goodreads webpage\n",
    "    :return: save a csv document with all information books\n",
    "    '''\n",
    "    base_url = 'https://www.goodreads.com'\n",
    "\n",
    "    for i in range(1,12):\n",
    "        df = pd.DataFrame(columns=[\"url\", \"title\", \"author\", \"num_reviews\", \"num_ratings\", \"avg_rating\", \"num_pages\",\n",
    "                                   \"original_publish_year\", \"series\", \"genres\", \"awards\", \"places\"])\n",
    "        page = requests.get(f'https://www.goodreads.com/list/show/29013.Best_Biographies_?page={i}')\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        book_titles = soup.find_all('a', class_=\"bookTitle\")\n",
    "        for book in book_titles:\n",
    "            try:\n",
    "                id_book = book.get('href')\n",
    "                data_to_append = bookscraper(base_url+id_book)\n",
    "                df_to_append = pd.DataFrame(data_to_append, index=[0])\n",
    "                df = df.append(df_to_append, ignore_index=True)\n",
    "            except:\n",
    "                pass\n",
    "        df.to_csv(path_or_buf='book_data.csv', mode='a', sep='&', header=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    data = pd.read_csv(data, sep='&') # Reading the data set\n",
    "    # Data cleanimg \n",
    "    data['original_publish_year'] = pd.to_numeric(data['original_publish_year'], errors='coerce')\n",
    "    data = data[data['original_publish_year']>1900]\n",
    "    #Count non null values in a pandas dataframe\n",
    "    \n",
    "    \n",
    "    data = data.dropna(subset=['avg_rating','original_publish_year'])\n",
    "    data = data[data['original_publish_year'] <= 2021]\n",
    "    data = data.drop_duplicates(subset=['title'])\n",
    "    data = data.reset_index(drop=True)\n",
    "    data['genres'].fillna(\"No Genre\", inplace = True)\n",
    "    #print(data.describe)\n",
    "    #print(data.isnull().sum())\n",
    "    #checking the Author column containing the numerice values count\n",
    "    authors_must_string = [not(str(author).isnumeric()) for author in data['author']]\n",
    "    data = data[authors_must_string]\n",
    "    #checking the Author column containing the numerice data and applying the simple indexing method\n",
    "    data.author.str.contains(r'[0-9]').value_counts()\n",
    "\n",
    "    #Converting the object data type to numerice data type\n",
    "    data['avg_rating'] = pd.to_numeric(data['avg_rating'],errors = 'coerce')\n",
    "\n",
    "    # MinMax Normilization on avg_rating and scaling from 0 to 10 and saving it into the minmax_norm_rating\n",
    "    data['minmax_norm_rating'] = 1 + (data['avg_rating'] - data['avg_rating'].min()) / (data['avg_rating'].max() - data['avg_rating'].min()) * 9\n",
    "    # Mean normilization\n",
    "    data['mean_norm_rating'] = 1 + (data['avg_rating'] - data['avg_rating'].mean()) / (data['avg_rating'].max() - data['avg_rating'].min()) * 9\n",
    "    # Converting the awards column into numerical data\n",
    "    data['awards'] = data['awards'].str.split(';').str.len()\n",
    "\n",
    "    data = data.rename(columns={'title': 'Title', 'original_publish_year': 'Publication year', 'minmax_norm_rating': 'Rating', 'awards':'Awards', 'num_pages':'NÂº pages', 'series':'Series'})\n",
    "    \n",
    "    \n",
    "    return(data)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #scraper() #To run the scraper Function\n",
    "    data = preprocessing('./book_data.csv')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9134d842bdcd315c0236068128f9507982a421bdbb3f601d119036447ad6d994"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('strive': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
